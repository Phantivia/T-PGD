{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Victim models from huggingface model hub\n",
    "model_paths = ['textattack/bert-base-uncased-ag-news',\n",
    "            'textattack/bert-base-uncased-MNLI',\n",
    "            'textattack/bert-base-uncased-SST-2',\n",
    "\n",
    "            'textattack/albert-base-v2-ag-news',\n",
    "            'textattack/albert-base-v2-SST-2',\n",
    "            'textattack/roberta-base-MNLI',\n",
    "            \"howey/bert-base-uncased-mnli\",\n",
    "            'textattack/roberta-base-ag-news',\n",
    "            'textattack/roberta-base-SST-2',\n",
    "            'Alireza1044/albert-base-v2-mnli']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(f\"nohup python RunTPGD.py \\\n",
    "                --task sst2\\\n",
    "                --tokenizer_checkpoint bert-base-uncased\\\n",
    "                --decode_mode Bert \\\n",
    "                --model_checkpoint  textattack/bert-base-uncased-SST-2 \\\n",
    "                    \\\n",
    "                --victim_model_checkpoint textattack/roberta-base-SST-2\\\n",
    "                --victim_tokenizer_checkpoint textattack/roberta-base-SST-2\\\n",
    "                    \\\n",
    "                --data_local_path ~/datasets/sst2\\\n",
    "                --start 0  --end 912\\\n",
    "                --cuda_device 0 --victim_device -1\\\n",
    "                --perturb_layer 0 --decode_layer 10\\\n",
    "                --num_seg_steps 100 --num_adv_steps 20\\\n",
    "                --adv_lr5 --init_mag 3 --decode_weight -0.5\\\n",
    "                --bs_lower_limit 0.30 --bs_upper_limit 0.95\\\n",
    "                --target_metric use\\\n",
    "                --eval_lower_limit 0.60\\\n",
    "                >SST2Exp/BertOnRoberta-SST2-Layer-Adv-Dw-USE-FULL.out 2>&1 &\")\n",
    "\n",
    "## Parameters Description ##\n",
    "\n",
    "# task: Task to run PandR on\n",
    "\n",
    "# tokenizer_checkpoint: Tokenizer used to build encoded dataset\n",
    "\n",
    "# decode_mode: Specify the behavior mode of prober, should be correspond to the type of victim model\n",
    "\n",
    "# model_checkpoint: Model name used in initializing model with AutoModel.from_pretrained()\n",
    "\n",
    "# victim_model_checkpoint victim model name used in initializing model with AutoModel.from_pretrained()\n",
    "\n",
    "# vvictim_tokenizer_checkpoint: Tokenizer used to tokenize adv samples to feed in victim model\n",
    "\n",
    "# data_local_path: Local path to the dataset saved with Dataset.save_to_disk()\n",
    "\n",
    "# start / end: Run attack on dataset[start:end]\n",
    "\n",
    "# cuda_device: Specify CUDA device. CPU mode is not supported yet.\n",
    "\n",
    "# victim_device: Default value -1 means initializing victim model on the same CUDA device as inferrence model.\n",
    "# Set to a valid value will move the vectim model to sepecific CUDA device\n",
    "# Designed for GPU memory insufficient cases\n",
    "\n",
    "# perturb_layer: The index of layer to add perturbation to. 0 performs best in most cases.\n",
    "\n",
    "# decode_layer: The index of layer to do hidden presentation reconstruction on.\n",
    "\n",
    "# num_seg_steps: Maximum number of iterations allowed for each input sample. \n",
    "# Corresponds to MaxIter in the paper.\n",
    "\n",
    "# num_adv_steps: Maximum steps of perturbation accumulation allowed in one iteration.\n",
    "# Corresponds to MaxStep in the paper.\n",
    "\n",
    "# adv_lr: The 2-norm value of a single perturbation, size of an adversarial perturbation step.\n",
    "\n",
    "# init_mag: The 2-norm value of the random initialized perturbation.\n",
    "\n",
    "# decode_weight: The value of β described in the paper. Loss = task_Loss + β * decode_Loss\n",
    "# Designed to keep the adversarial text from straying too far from original text.\n",
    "\n",
    "# bs_lower_limit: The lowest USE score between original & adversarial text. \n",
    "# Sample that gets lower USE score than this value will be ignored.\n",
    "\n",
    "# bs_upper_limit: The highest USE score between original & adversarial text. \n",
    "# Sample that gets higher USE score than this value will trigger an early-stop of the attack process.\n",
    "\n",
    "# seed: Default value: 114514. Random seed used in attack process.\n",
    "\n",
    "## Extra Parameters Description ##\n",
    "\n",
    "# local_victim: Path to loacally saved victim model. Model should have been saved with torch.save(model, path)\n",
    "\n",
    "# local_model: Path to loacally saved inferrence model. Model should have been saved with torch.save(model, path)\n",
    "\n",
    "# stop_random_cover: Default value: False. Set this to True to stop replacing random token with [MASK] in the beginning of each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BERT on Roberta SST2\n",
    "device = 1\n",
    "for layer in [10]:\n",
    "    for adv_lr in [5]:\n",
    "        for dw in [0.5]:\n",
    "            os.system(f\"nohup python RunTPGD.py \\\n",
    "                --task sst2\\\n",
    "                --tokenizer_checkpoint bert-base-uncased\\\n",
    "                --decode_mode Bert \\\n",
    "                --model_checkpoint  textattack/bert-base-uncased-SST-2 \\\n",
    "                    \\\n",
    "                --victim_model_checkpoint textattack/roberta-base-SST-2\\\n",
    "                --victim_tokenizer_checkpoint textattack/roberta-base-SST-2\\\n",
    "                    \\\n",
    "                --data_local_path ~/datasets/sst2\\\n",
    "                --start 0  --end 912\\\n",
    "                --cuda_device {device} --victim_device -1\\\n",
    "                --perturb_layer 0 --decode_layer {layer}\\\n",
    "                --num_seg_steps 100 --num_adv_steps 20\\\n",
    "                --adv_lr {adv_lr} --init_mag 3 --decode_weight -{dw}\\\n",
    "                --bs_lower_limit 0.30 --bs_upper_limit 0.95\\\n",
    "                --target_metric use\\\n",
    "                --eval_lower_limit 0.60\\\n",
    "                >SST2Exp/BertOnRoberta-SST2-Layer{layer}-Adv{adv_lr}-Dw{dw}-USE-FULL.out 2>&1 &\")\n",
    "            import time\n",
    "            device += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BERT on Albert SST2\n",
    "device = 1\n",
    "for layer in [10]:\n",
    "    for adv_lr in [5]:\n",
    "        for dw in [0.5]:\n",
    "            os.system(f\"nohup python RunTPGD.py \\\n",
    "                --task sst2\\\n",
    "                --tokenizer_checkpoint bert-base-uncased\\\n",
    "                --decode_mode Bert \\\n",
    "                --model_checkpoint  textattack/bert-base-uncased-SST-2 \\\n",
    "                    \\\n",
    "                --victim_model_checkpoint textattack/albert-base-v2-SST-2\\\n",
    "                --victim_tokenizer_checkpoint textattack/albert-base-v2-SST-2\\\n",
    "                    \\\n",
    "                --data_local_path ~/datasets/sst2\\\n",
    "                --start 0  --end 100\\\n",
    "                --cuda_device {device} --victim_device -1\\\n",
    "                --perturb_layer 0 --decode_layer {layer}\\\n",
    "                --num_seg_steps 100 --num_adv_steps 20\\\n",
    "                --adv_lr {adv_lr} --init_mag 3 --decode_weight -{dw}\\\n",
    "                --bs_lower_limit 0.30 --bs_upper_limit 0.95\\\n",
    "                --target_metric use\\\n",
    "                --eval_lower_limit 0.75\\\n",
    "                >SST2Exp/100-BertOnAlbert-SST2-Layer{layer}-Adv{adv_lr}-Dw{dw}-USE-FULL.out 2>&1 &\")\n",
    "            import time\n",
    "            device += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BERT on Bert SST2\n",
    "device = 0\n",
    "for layer in [10]:\n",
    "    for adv_lr in [5]:\n",
    "        for dw in [0.5]:\n",
    "            os.system(f\"nohup python RunTPGD.py \\\n",
    "                --task sst2\\\n",
    "                --tokenizer_checkpoint bert-base-uncased\\\n",
    "                --decode_mode Bert \\\n",
    "                --model_checkpoint  textattack/bert-base-uncased-SST-2 \\\n",
    "                    \\\n",
    "                --victim_model_checkpoint textattack/bert-base-uncased-SST-2\\\n",
    "                --victim_tokenizer_checkpoint textattack/bert-base-uncased-SST-2\\\n",
    "                    \\\n",
    "                --data_local_path ~/datasets/sst2\\\n",
    "                --start 0  --end 912\\\n",
    "                --cuda_device {device} --victim_device -1\\\n",
    "                --perturb_layer 0 --decode_layer {layer}\\\n",
    "                --num_seg_steps 100 --num_adv_steps 20\\\n",
    "                --adv_lr {adv_lr} --init_mag 3 --decode_weight -{dw}\\\n",
    "                --bs_lower_limit 0.30 --bs_upper_limit 0.95\\\n",
    "                --eval_lower_limit 0.70\\\n",
    "                --target_metric use\\\n",
    "                --local_model /home/phantivia/models/bert-base-uncased-SST-2.model\\\n",
    "                >SST2Exp/BertOnBert-SST2-Layer{layer}-Adv{adv_lr}-Dw{dw}-USE-FULL.out 2>&1 &\")\n",
    "\n",
    "            import time\n",
    "            device += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BERT on Roberta AG\n",
    "device = 0\n",
    "import time\n",
    "for layer in [8]:\n",
    "    device = 0\n",
    "    for adv_lr in [5, 10, 15]:\n",
    "        for dw in [0.5, 1.0]:\n",
    "            os.system(f\"nohup python RunTPGD.py \\\n",
    "                --task ag_news\\\n",
    "                --tokenizer_checkpoint bert-base-uncased\\\n",
    "                --decode_mode Bert \\\n",
    "                --model_checkpoint  textattack/bert-base-uncased-ag-news \\\n",
    "                    \\\n",
    "                --victim_model_checkpoint textattack/roberta-base-ag-news\\\n",
    "                --victim_tokenizer_checkpoint textattack/roberta-base-ag-news\\\n",
    "                    \\\n",
    "                --data_local_path ~/datasets/ag_news\\\n",
    "                --start 0  --end 100\\\n",
    "                --cuda_device {device} --victim_device -1\\\n",
    "                --perturb_layer 0 --decode_layer {layer}\\\n",
    "                --num_seg_steps 100 --num_adv_steps 20\\\n",
    "                --adv_lr {adv_lr} --init_mag 3 --decode_weight -{dw}\\\n",
    "                --bs_lower_limit 0.30 --bs_upper_limit 0.95\\\n",
    "                --target_metric use\\\n",
    "                >logs/BertOnRoberta-AG-Layer{layer}-Adv{adv_lr}-Dw{dw}-USE-100.out 2>&1 &\")\n",
    "            \n",
    "            device += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BERT on Albert AG\n",
    "device = 0\n",
    "import time\n",
    "for layer in [8, 9]:\n",
    "    for adv_lr in [10]:\n",
    "        for dw in [0.5, 1.0]:\n",
    "            os.system(f\"nohup python RunTPGD.py \\\n",
    "                --task ag_news\\\n",
    "                --tokenizer_checkpoint bert-base-uncased\\\n",
    "                --decode_mode Bert \\\n",
    "                --model_checkpoint  textattack/bert-base-uncased-ag-news \\\n",
    "                    \\\n",
    "                --victim_model_checkpoint textattack/albert-base-v2-ag-news\\\n",
    "                --victim_tokenizer_checkpoint textattack/albert-base-v2-ag-news\\\n",
    "                    \\\n",
    "                --data_local_path ~/datasets/ag_news\\\n",
    "                --start 0  --end 1000\\\n",
    "                --cuda_device {device} --victim_device -1\\\n",
    "                --perturb_layer 0 --decode_layer {layer}\\\n",
    "                --num_seg_steps 100 --num_adv_steps 20\\\n",
    "                --adv_lr {adv_lr} --init_mag 3 --decode_weight -{dw}\\\n",
    "                --bs_lower_limit 0.30 --bs_upper_limit 0.95\\\n",
    "                --target_metric use\\\n",
    "                >logs/BertOnAlbert-AG-Layer{layer}-Adv{adv_lr}-Dw{dw}-USE-100.out 2>&1 &\")\n",
    "            \n",
    "            device += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BERT on BERT AG\n",
    "device = 1\n",
    "import time\n",
    "for layer in [7]:\n",
    "    for adv_lr in [10]:\n",
    "        for dw in [0.5]:\n",
    "            os.system(f\"nohup python RunTPGD.py \\\n",
    "                --task ag_news\\\n",
    "                --tokenizer_checkpoint bert-base-uncased\\\n",
    "                --decode_mode Bert \\\n",
    "                --model_checkpoint  textattack/bert-base-uncased-ag-news \\\n",
    "                    \\\n",
    "                --victim_model_checkpoint textattack/bert-base-uncased-ag-news\\\n",
    "                --victim_tokenizer_checkpoint textattack/bert-base-uncased-ag-news\\\n",
    "                    \\\n",
    "                --data_local_path ~/datasets/ag_news\\\n",
    "                --start 0  --end 1000\\\n",
    "                --cuda_device {device} --victim_device -1\\\n",
    "                --perturb_layer 0 --decode_layer {layer}\\\n",
    "                --num_seg_steps 150 --num_adv_steps 15\\\n",
    "                --adv_lr {adv_lr} --init_mag 3 --decode_weight -{dw}\\\n",
    "                --bs_lower_limit 0.30 --bs_upper_limit 0.85\\\n",
    "                --target_metric use\\\n",
    "                --eval_lower_limit 0.50\\\n",
    "                --local_model ~/models/bert-base-uncased-ag-news.model\\\n",
    "                >SST2Exp/BertOnBert-AG-Layer{layer}-Adv{adv_lr}-Dw{dw}-USE-FULL.out 2>&1 &\")\n",
    "            \n",
    "            device += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BERT on Albert MNLI\n",
    "device = 1\n",
    "import time\n",
    "for layer in [11]:\n",
    "    for adv_lr in [1]:\n",
    "        for dw in [10]:\n",
    "            os.system(f\"nohup python RunTPGD.py \\\n",
    "                --task mnli_hypothesis\\\n",
    "                --tokenizer_checkpoint bert-base-uncased\\\n",
    "                --decode_mode Bert \\\n",
    "                --model_checkpoint  textattack/bert-base-uncased-MNLI \\\n",
    "                    \\\n",
    "                --victim_model_checkpoint Alireza1044/albert-base-v2-mnli\\\n",
    "                --victim_tokenizer_checkpoint albert-base-v2\\\n",
    "                    \\\n",
    "                --data_local_path ~/datasets/mnli\\\n",
    "                --start 0  --end 1000\\\n",
    "                --cuda_device {device} --victim_device -1\\\n",
    "                --perturb_layer 0 --decode_layer {layer}\\\n",
    "                --num_seg_steps 150 --num_adv_steps 20\\\n",
    "                --adv_lr {adv_lr} --init_mag 3 --decode_weight -{dw}\\\n",
    "                --bs_lower_limit 0.30 --bs_upper_limit 0.95\\\n",
    "                --target_metric use\\\n",
    "                --eval_lower_limit 0.7\\\n",
    "                >SST2Exp/BertOnAlbert-MNLI-Layer{layer}-Adv{adv_lr}-Dw{dw}-BS-FULL.out 2>&1 &\")\n",
    "            \n",
    "            device += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BERT on Bert MNLI\n",
    "device = 0\n",
    "import time\n",
    "for layer in [12]:\n",
    "    for adv_lr in [1]:\n",
    "        for dw in [10]:\n",
    "            os.system(f\"nohup python RunTPGD.py \\\n",
    "                --task mnli_hypothesis\\\n",
    "                --tokenizer_checkpoint bert-base-uncased\\\n",
    "                --decode_mode Bert \\\n",
    "                --model_checkpoint  ishan/bert-base-uncased-mnli\\\n",
    "                    \\\n",
    "                --victim_model_checkpoint textattack/bert-base-uncased-MNLI\\\n",
    "                --victim_tokenizer_checkpoint bert-base-uncased\\\n",
    "                    \\\n",
    "                --data_local_path ~/datasets/mnli\\\n",
    "                --start 0  --end 1000\\\n",
    "                --cuda_device {device} --victim_device -1\\\n",
    "                --perturb_layer 0 --decode_layer {layer}\\\n",
    "                --num_seg_steps 150 --num_adv_steps 20\\\n",
    "                --adv_lr {adv_lr} --init_mag 3 --decode_weight -{dw}\\\n",
    "                --bs_lower_limit 0.30 --bs_upper_limit 0.95\\\n",
    "                --target_metric use\\\n",
    "                --eval_lower_limit 0.6\\\n",
    "                >SST2Exp/BertOnBert-MNLI-Layer{layer}-Adv{adv_lr}-Dw{dw}-USE-FULL.out 2>&1 &\")\n",
    "            \n",
    "            device += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BERT on ROBERTA MNLI\n",
    "device = 1\n",
    "import time\n",
    "for layer in [11]:\n",
    "    for adv_lr in [1]:\n",
    "        for dw in [10]:\n",
    "            os.system(f\"nohup python RunTPGD.py \\\n",
    "                --task mnli_hypothesis\\\n",
    "                --tokenizer_checkpoint bert-base-uncased\\\n",
    "                --decode_mode Bert \\\n",
    "                --model_checkpoint  textattack/bert-base-uncased-MNLI \\\n",
    "                    \\\n",
    "                --victim_model_checkpoint textattack/roberta-base-MNLI\\\n",
    "                --victim_tokenizer_checkpoint roberta-base\\\n",
    "                    \\\n",
    "                --data_local_path ~/datasets/mnli\\\n",
    "                --start 0  --end 1000\\\n",
    "                --cuda_device {device} --victim_device -1\\\n",
    "                --perturb_layer 0 --decode_layer {layer}\\\n",
    "                --num_seg_steps 200 --num_adv_steps 20\\\n",
    "                --adv_lr {adv_lr} --init_mag 3 --decode_weight -{dw}\\\n",
    "                --bs_lower_limit 0.30 --bs_upper_limit 0.95\\\n",
    "                --target_metric use\\\n",
    "                --eval_lower_limit 0.4\\\n",
    "                >SST2Exp/BertOnRoberta-MNLI-Layer{layer}-Adv{adv_lr}-Dw{dw}-BS-FULL.out 2>&1 &\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d3e077174452397e51022fc37ea4e70a986c1a8dd7bc5d5e567b6349c9bf9e11"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('phantivia': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
